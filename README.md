# Fabrik II Demo - Comprehensive Chaos Engineering for Dynatrace

A full-stack microservices application designed to demonstrate Dynatrace Davis AI anomaly detection, root cause analysis, and deployment correlation through realistic failure scenarios.

## ğŸ—ï¸ Architecture

```mermaid
flowchart TD
    subgraph "External Load Generator"
        LG[Fab Proxy]
    end

    subgraph "Fabrik Microservices Platform"
        FE["Frontend Service<br/>ğŸ”¥ HTTP 500s<br/>â±ï¸ Slow Responses"]
        OS["Orders Service<br/>ğŸ”¥ DB Timeouts<br/>â±ï¸ 5s Delays"]
        FS["Fulfillment Service<br/>ğŸ”¥ DB Timeouts<br/>â±ï¸ Query Failures"]
        IS["Inventory Service<br/>ğŸ”¥ DB Timeouts<br/>â™»ï¸ Auto-Replenish"]
        SR["Shipping Receiver<br/>ğŸ”¥ Message Failures<br/>â±ï¸ Connection Timeouts"]
        SP["Shipping Processor<br/>ğŸ”¥ gRPC Failures<br/>ğŸ”¥ DB Timeouts"]

        DB[(Postgres Database<br/>ğŸ¯ Failure Target)]
        K["Kafka Broker<br/>ğŸ“¨ Message Queue"]
    end

    %% Load Generation Flow
    LG -- "ğŸš€ HTTP POST /order<br/>Continuous Load" --> FE

    %% Frontend Flow
    FE -- "ğŸ“Š JDBC Query (Check Orders)<br/>ğŸ’¥ 15% Failure Rate" --> DB
    FE -- "ğŸ”— gRPC PlaceOrder<br/>ğŸ’¥ HTTP 500 Injection" --> OS

    %% Orders Flow
    OS -- "ğŸ’¾ JDBC Insert (New Order)<br/>ğŸ’¥ 30% Failure Rate" --> DB
    OS -- "ğŸ“¤ Publish 'orders' Topic<br/>ğŸ’¥ DB Timeout Chain" --> K

    %% Fulfillment Flow (Async)
    K -- "ğŸ“¥ Consume 'orders'<br/>ğŸ’¥ 30% Failure Rate" --> FS
    FS -- "ğŸ“ JDBC Update (Fraud Check)<br/>ğŸ’¥ Query Timeouts" --> DB

    %% Inventory Flow (Async)
    K -- "ğŸ“¥ Consume 'orders'<br/>ğŸ’¥ 25% Failure Rate" --> IS
    IS -- "ğŸ“¦ JDBC Select/Update (Stock)<br/>ğŸ’¥ Slow Lookups" --> DB
    IS -- "ğŸ“¤ Publish 'inventory-reserved'<br/>â™»ï¸ Auto-Replenishment" --> K

    %% Shipping Flow (Async + Sync)
    K -- "ğŸ“¥ Consume 'inventory-reserved'<br/>ğŸ’¥ 20% Failure Rate" --> SR
    SR -- "ğŸ”— gRPC ShipOrder<br/>ğŸ’¥ Connection Failures" --> SP
    SP -- "ğŸ’¾ JDBC Insert (Shipment)<br/>ğŸ’¥ 20% Failure Rate" --> DB

    %% Chaos Indicators
    subgraph "ğŸ”¥ Chaos Engineering"
        direction TB
        C1["âš¡ Random Intervals: 0-2 Hours"]
        C2["ğŸ’¥ Correlated Failures Across Stack"]
        C3["ğŸ¯ Multiple Failure Types"]
        C4["ğŸ“Š Davis AI Anomaly Detection"]
    end

    %% Styling
    classDef service fill:#e1f5fe,stroke:#01579b,stroke-width:2px;
    classDef chaos fill:#ffebee,stroke:#d32f2f,stroke-width:2px;
    classDef db fill:#f3e5f5,stroke:#4a148c,stroke-width:2px;
    classDef queue fill:#fff3e0,stroke:#e65100,stroke-width:2px;

    class FE,OS,FS,IS,SR,SP service;
    class DB db;
    class K queue;
    class C1,C2,C3,C4 chaos;
```

## ğŸ”¥ Advanced Chaos Engineering

This demo implements comprehensive chaos engineering to create realistic production-like failures that demonstrate Dynatrace's AI-powered observability capabilities.

### ğŸ¯ **Demo Objectives**
- **Anomaly Detection**: Unpredictable failure patterns that Davis AI can identify as deviations from baseline
- **Root Cause Analysis**: Correlated failures across the microservices stack
- **Deployment Impact**: Clear correlation between deployment events and system degradation
- **End-to-End Visibility**: Full transaction tracing through cascading failures

### âš¡ **Chaos Simulation Features**

#### **1. Unpredictable Timing**
- **Random Intervals**: 0-2 hours between chaos episodes
- **Prevents Baseline Adaptation**: Davis AI doesn't normalize failures as expected behavior
- **Realistic Production Patterns**: Mimics real-world unpredictable outages

#### **2. Multi-Service Failure Injection**

| Service | Failure Rate | Failure Types | Impact |
|---------|-------------|---------------|--------|
| **Orders** | 30% | DB query timeouts, 5s delays | Core transaction blocking |
| **Fulfillment** | 30% | DB connection issues, query failures | Fraud check delays |
| **Inventory** | 25% | Slow lookups, timeout exceptions | Stock check failures |
| **Shipping Receiver** | 20% | Message processing failures | Order fulfillment breaks |
| **Shipping Processor** | 20% | gRPC errors, DB timeouts | Shipment creation fails |
| **Frontend** | 15% | HTTP 500 responses, slow responses | User experience degradation |

#### **3. Realistic Failure Scenarios**

**Database Connection Pool Exhaustion:**
```
ğŸ”¥ Simulated via query timeouts across all services
ğŸ“Š Shows cascading database pressure
ğŸ¯ Demonstrates infrastructure bottlenecks
```

**End-to-End Transaction Failures:**
```
ğŸš€ Frontend Request â†’ ğŸ’¥ Orders Timeout â†’ ğŸ’¥ Inventory Delay â†’ ğŸ’¥ Shipping Failure
ğŸ“ˆ Complete user journey degradation
ğŸ” Perfect for distributed tracing analysis
```

**Multi-Protocol Communication Failures:**
```
ğŸ“¡ HTTP (Frontend APIs)
ğŸ”— gRPC (Orders â†” Shipping)
ğŸ“¨ Kafka Messaging (All async flows)
ğŸ’¾ JDBC Database (All services)
```

#### **4. Continuous Operation Features**

**Auto-Inventory Replenishment:**
- Automatically restocks when inventory â‰¤ 5 items
- Ensures shipping services always have work
- Maintains continuous demo flow

**Deployment Event Correlation:**
- SDLC events mark chaos periods
- Clear deploymentâ†’failure correlation
- Enables deployment impact analysis

### ğŸš€ **Running Chaos Simulations**

#### **Manual Trigger:**
```bash
kubectl exec -n default -it deploy/argo -- /app/simulate.sh manual
```

#### **Automatic Operation:**
The chaos simulation runs continuously with random intervals:
- **Timing**: 0-2 hours between episodes
- **Duration**: 10 minutes of coordinated failures
- **Recovery**: Automatic rollback to stable state

#### **Expected Dynatrace Observations:**

**During Chaos Episodes:**
- ğŸ”º Response time increases across all services
- ğŸš¨ Error rate spikes (HTTP 500s, DB exceptions)
- ğŸ“Š Database query timeout alerts
- ğŸ”— Broken distributed traces
- ğŸ“‰ Transaction failure rate increases

**Davis AI Analysis:**
- ğŸ¤– Anomaly detection for performance deviations
- ğŸ¯ Root cause correlation to deployment events
- ğŸ•·ï¸ Service dependency impact mapping
- ğŸ“ˆ Baseline vs. incident performance comparison

### ğŸ“Š **Perfect Demo Scenarios**

1. **Deployment Impact Analysis**: Show how "v2.0.0-green" deployment correlates with system degradation
2. **AI-Powered Root Cause**: Davis identifies database timeouts as primary failure cause
3. **Service Dependency Mapping**: Visualize how failures cascade through microservices
4. **Automated Remediation**: Demonstrate rollback to "v1.0.0-blue" restoring system health
5. **Proactive Monitoring**: Alert on performance degradation before complete failure

This comprehensive chaos engineering setup provides rich, realistic data for demonstrating Dynatrace's full observability and AI capabilities in a modern microservices environment.
